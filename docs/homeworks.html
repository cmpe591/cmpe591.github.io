<!doctype html>
<html lang="en">
<head>
    <title>CMPE591: Deep Learning in Robotics</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
    <link href="css/styles.css" rel="stylesheet">
</head>

<body>
    <div class="w3-container w3-margin-bottom" style="max-width: 960px; margin: auto">
        <header class="">
            <h1 class="w3-center">CMPE591: Deep Learning in Robotics</h1>
            <nav class="w3-bar">
                <a class="w3-bar-item w3-button" href="index.html">Syllabus</a>
                <a class="w3-bar-item w3-button" href="homeworks.html">Homeworks</a>
            </nav>
        </header>
        <details open>
            <summary><h2>Preparing the Environment</h2></summary>
            <div class="w3-row">
                It is suggested that you install a virtual environment for this course. You can use Anaconda or Miniconda (smaller size) for this purpose. You can download Anaconda from <a href="https://www.anaconda.com/products/distribution">here</a>. Alternatively, you can use Mamba (a faster version of conda) for this purpose. You can download Mamba from <a href="https://github.com/conda-forge/miniforge#mambaforge">here</a>. Install the downloaded script by running the following command in your terminal:
                <pre>$ bash &ltdownloaded_script&gt.sh</pre>
                After the installation, you can create a virtual environment by running the following command:
<pre>
# For Anaconda
$ conda create -n &ltvirtual_environment_name&gt python=3.9
$ conda activate &ltvirtual_environment_name&gt

# For Mamba
$ mamba create -n &ltvirtual_environment_name&gt python=3.9
$ mamba activate &ltvirtual_environment_name&gt
</pre>
                You will need to run <code>mamba activate &ltvirtual_environment_name&gt</code> (or <code>conda</code>) every time you open a new terminal to activate the virtual environment. You can deactivate the virtual environment by running <code>mamba deactivate</code>.<br><br>
                We will use <a href="https://mujoco.org">MuJoCo</a> and <a href="https://github.com/deepmind/dm_control">dm_control</a> for our simulation environment. You can install them by running:
<pre>
$ pip install dm_control  # dm_control automatically installs mujoco
</pre>
                also install some other dependencies:
<pre>
$ pip install mujoco-python-viewer
$ pip install pyyaml
</pre>
                Additionally, we will use PyTorch for training our models, check out the <a href="https://pytorch.org/get-started/locally/">installation instructions</a>. After installing PyTorch, clone the homework repository by running:
<pre>
$ git clone https://github.com/cmpe591/cmpe591.github.io.git
</pre>
Homeworks will be released in the <code>src</code> folder of the repository. You can run the demo code by running:
<pre>
$ cd cmpe591.github.io/src
$ python demo.py
</pre>
                You should see the following output:
                <img class="w3-margin-top w3-margin-bottom" src="images/hw1.png" alt="homework1" style="width: 100%">
                <code>environment.py</code> and <code>mujoco_menagerie</code> will be common throughout homeworks and <code>homework&ltx&gt.py</code> will be added each week.
    It is suggested that you use <a href="https://code.visualstudio.com/">Visual Studio Code</a> with GitHub Copilot for easier development (though double-check everything that copilot suggests). GitHub Copilot is free for students.
            </div>
        </details>
        <details open>
            <summary><h2>Homework 1 (Training a DNN using PyTorch)</h2></summary>
            <div class="w3-row">
                In this homework, you will train a deep neural network that estimates the object's position given the executed action and the state (a top-down view of the environment). Below are some example states.
                <div class="w3-container w3-center w3-margin-top w3-margin-bottom">
                <img src="images/hw1_states.png" alt="homework1" style="width: 80%">
                </div>
                There are two object types (cube and sphere) with random sizes between 2cm to 3cm, and the robot randomly pushes the object to four main directions. Based on the object's type and size, the resulting object position changes. Assuming that you have already cloned the repository, you can run the following code for sampling the data:
<pre>
import numpy as np
from homework1 import Hw1Env

env = Hw1Env(render_mode="gui")
for _ in range(100):
    env.reset()
    action_id = np.random.randint(4)
    _, img_before = env.state()
    env.step(action_id)
    pos_after, img_after = env.state()
    env.reset()
</pre>
                You might also want to check the main part of the <code>homework1.py</code> to see how to collect data with multiple processes. Sample 1000 data point and train
                <ol>
                    <li>A plain multi-layer perceptron (MLP)</li>
                    <li>Convolutional neural network</li>
                </ol>
                using PyTorch.
                <h3>Optional</h3>
                Instead of predicting the object's position, predict the raw image of the environment after the action.
            </div>
        </details>
    </div>
</body>

</html>